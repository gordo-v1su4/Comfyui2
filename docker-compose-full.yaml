version: '3.8'

# Shared network for all AI services
networks:
  ai-network:
    driver: bridge
    external: false

# Named volumes for persistent data
volumes:
  minio-data:
    driver: local
  shared-ai-models:
    driver: local

services:
  # MinIO S3-compatible storage
  minio:
    image: 'quay.io/minio/minio:latest'
    container_name: minio-storage
    command: 'server /data --console-address ":9001"'
    environment:
      - MINIO_SERVER_URL=${MINIO_SERVER_URL}
      - MINIO_BROWSER_REDIRECT_URL=${MINIO_BROWSER_REDIRECT_URL}
      - MINIO_ROOT_USER=${SERVICE_USER_MINIO}
      - MINIO_ROOT_PASSWORD=${SERVICE_PASSWORD_MINIO}
    volumes:
      - 'minio-data:/data'
    ports:
      - "9000:9000"   # API
      - "9001:9001"   # Console
    networks:
      - ai-network
    healthcheck:
      test:
        - CMD
        - mc
        - ready
        - local
      interval: 5s
      timeout: 20s
      retries: 10
    restart: unless-stopped

  # MinIO bucket initializer
  minio-init:
    image: minio/mc:latest
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    networks:
      - ai-network
    environment:
      - MINIO_ENDPOINT=http://minio:9000
      - MINIO_ACCESS_KEY=${SERVICE_USER_MINIO}
      - MINIO_SECRET_KEY=${SERVICE_PASSWORD_MINIO}
    entrypoint: |
      /bin/sh -c "
      echo 'ðŸ”§ Initializing MinIO buckets for AI models...'
      
      # Configure MinIO client
      mc alias set myminio http://minio:9000 \$$MINIO_ACCESS_KEY \$$MINIO_SECRET_KEY
      
      # Create buckets
      mc mb myminio/ai-models --ignore-existing
      mc mb myminio/ai-outputs --ignore-existing
      mc mb myminio/ai-workflows --ignore-existing
      
      # Set bucket policies (public read for models)
      mc anonymous set public myminio/ai-models
      mc anonymous set private myminio/ai-outputs
      mc anonymous set public myminio/ai-workflows
      
      # Create folder structure
      echo '# AI Models folder structure' | mc pipe myminio/ai-models/models/.keep
      echo '# Custom nodes folder' | mc pipe myminio/ai-models/custom_nodes/.keep
      echo '# Workflows folder' | mc pipe myminio/ai-workflows/comfyui/.keep
      echo '# Shared outputs folder' | mc pipe myminio/ai-outputs/shared/.keep
      
      # Create ComfyUI model subdirectories
      for dir in checkpoints vae loras controlnet embeddings upscale_models clip_vision diffusers photomaker insightface faceanalysis style_models ipadapter instantid pulid; do
        echo \"# \$dir models directory\" | mc pipe myminio/ai-models/models/\$dir/.keep
        echo \"   âœ“ Created: models/\$dir/\"
      done
      
      echo 'âœ… MinIO bucket initialization complete!'
      echo 'ðŸ“Š Created buckets: ai-models, ai-outputs, ai-workflows'
      echo 'ðŸ”— Models will be shared across all ComfyUI instances'
      "
    restart: "no"

  # ComfyUI Easy Install with S3 integration
  comfyui:
    build: .
    container_name: comfyui-easy-install
    depends_on:
      minio-init:
        condition: service_completed_successfully
    ports:
      - "8188:8188"
    networks:
      - ai-network
    volumes:
      # Instance-specific local storage
      - ./output:/app/ComfyUI-Easy-Install/ComfyUI/output
      - ./input:/app/ComfyUI-Easy-Install/ComfyUI/input
      - ./temp:/app/ComfyUI-Easy-Install/ComfyUI/temp
      - ./user:/app/ComfyUI-Easy-Install/ComfyUI/user
      
      # S3 integration via host mounts (Coolify will override these)
      - shared-ai-models:/app/ComfyUI-Easy-Install/ComfyUI/models
    environment:
      - PYTHONUNBUFFERED=1
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      
      # S3/MinIO configuration for ComfyUI Manager
      - COMFYUI_S3_ENABLED=true
      - MINIO_ENDPOINT=http://minio:9000
      - MINIO_ACCESS_KEY=${SERVICE_USER_MINIO}
      - MINIO_SECRET_KEY=${SERVICE_PASSWORD_MINIO}
      - MINIO_BUCKET_MODELS=ai-models
      
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8188/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia

  # Optional: Second ComfyUI instance (for testing shared storage)
  comfyui-secondary:
    build: .
    container_name: comfyui-secondary
    depends_on:
      minio-init:
        condition: service_completed_successfully
    ports:
      - "8189:8188"  # Different port
    networks:
      - ai-network
    volumes:
      # Instance-specific local storage
      - ./output-secondary:/app/ComfyUI-Easy-Install/ComfyUI/output
      - ./input:/app/ComfyUI-Easy-Install/ComfyUI/input
      - ./temp-secondary:/app/ComfyUI-Easy-Install/ComfyUI/temp
      - ./user-secondary:/app/ComfyUI-Easy-Install/ComfyUI/user
      
      # Shared S3 storage (same as primary instance)
      - shared-ai-models:/app/ComfyUI-Easy-Install/ComfyUI/models
    environment:
      - PYTHONUNBUFFERED=1
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256  # Less memory for secondary
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - COMFYUI_S3_ENABLED=true
      - MINIO_ENDPOINT=http://minio:9000
      - MINIO_ACCESS_KEY=${SERVICE_USER_MINIO}
      - MINIO_SECRET_KEY=${SERVICE_PASSWORD_MINIO}
      - MINIO_BUCKET_MODELS=ai-models
    restart: unless-stopped
    profiles:
      - secondary  # Optional profile - start with: docker-compose --profile secondary up
    runtime: nvidia
